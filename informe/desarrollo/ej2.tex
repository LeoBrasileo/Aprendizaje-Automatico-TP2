\section*{Árboles de decisión}

Este ejercicio consistía de la construcción de modelos de tipo árbol de decisión, y luego realizar una estimación de performance de estos.
El primer objetivo realizado fue construir un arbol con altura máxima 3 con sus parametros en default. Luego, realizamos un K-fold cross validation sobre este utilizando distintas métricas (Accuracy, AUC-PRC y AUC-ROC). La idea era calcular las métricas utilizando tanto el promedio de los resultados de cada fold, como también el score global para los folds de validación, utilizando siempre los mismos folds.

\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\hline
    & \textbf{Acc. (Val)} & \textbf{Acc. (Train)} & \textbf{AUC ROC (Val)} & \textbf{AUC ROC (Train)} & \textbf{AUC PRC (Val)} & \textbf{AUC PRC (Train)} \\
\hline
0 & 0.588235 & 0.823529 & 0.520333 & 0.846545 & 0.311156 & 0.650633 \\
1 & 0.670588 & 0.802941 & 0.636533 & 0.822029 & 0.337906 & 0.673870 \\
2 & 0.647059 & 0.826471 & 0.584827 & 0.839358 & 0.445933 & 0.638929 \\
3 & 0.670588 & 0.820588 & 0.649425 & 0.840051 & 0.418101 & 0.675693 \\
4 & 0.658824 & 0.794118 & 0.664276 & 0.836437 & 0.375702 & 0.681502 \\
\hline
\textbf{Promedios} & 0.647059 & 0.813529 & 0.611079 & 0.836884 & 0.377759 & 0.664125 \\
\textbf{Global} & 0.647059 & -- & 0.507207 & -- & 0.305654 & -- \\
\hline
\end{tabular}
}
\caption*{Resultados de validación cruzada}
\label{tab:resultados_cv}
\end{table}

Analicemos los resultados para cada métrica, considerando el modelo usado, la cantidad de atributos de nuestras instancias, y el desbalance de clases que tienen los datos.
\begin{itemize}
    \item \textbf{Accuracy} (Acc.): notemos que su valor en datos de validación es $\approx 0.65$. Pensando en que los datos de entrenamiento tienen 69,8\% de instancias de clase negativa, este resultado es peor que lo que conseguiría un clasificador que siempre predice la clase más frecuente. En función de esta métrica, se sugiere que el modelo no está aprendiendo demasiado de los datos. 
    \item \textbf{AUC-ROC}: de la misma forma, un clasificador que predice siempre la clase más frecuente tendría un AUC-ROC de $0.50$, y nuestro modelo lo supera apenas con $\approx 0.6110$ con el promedio de sus folds, y tiene aproximadamente el mismo score en el valor global.
    \item \textbf{AUC-PRC}: tenemos un resultado análogo. En este caso, los valores absolutos son más bajos aún, y tiene sentido considerando que AUC-PRC sólo se calcula en función de Precision y Recall: estamos teniendo en cuenta sólo los valores de false positive, false negative, true positive. Como nuestros datos de entrenamiento están desbalanceados a favor de la clase negativa, que esta métrica que no tenga en cuenta los valores de true negative afecta bastante el resultado. Un clasificador que nos da siempre la clase más frecuente tendría valor  $\approx 0.30$ de AUC PRC, correspondido a nuestra proporción de 30.2\% de instancias positivas. El valor que nos dio está por debajo de ese valor en cuanto a validación, pero es un poco mayor en entrenamiento. 
     
\end{itemize}

En cada caso, vemos que los puntajes en datos de entrenamiento superan ampliamente lo que sería elecciones aleatorias o de clase más frecuente para cada métrica. La diferencia con los valores durante la validación nos sugiere sobreajuste del modelo. La causa más probable es que las instancias tienen 200 atributos y el árbol evaluado tiene como límite altura 3. Con esta limitación, al modelo le falta complejidad para capturar patrones complejos de los datos, pudiendo aprender instancias ya vistas, pero teniendo baja capacidad de generalizar a instancias nuevas.

\subsection*{Exploración de hipérparametros}	
Luego de realizar la validación cruzada, el siguiente paso fue realizar una búsqueda de hipérparametros. Para ello, volvimos a hacer un K-fold cross validation, pero esta vez con un Grid Search de los parámetros de altura máxima y el criterio de corte utilizado (Gini y Entropia). Guardamos el score conseguido con cada combinación de parámetros y chequeamos su Accuracy tanto en training como en validación. La siguiente tabla muestra los resultados obtenidos.

\begin{table}[H]
    \centering
    \begin{tabular}{cccc}
    \hline
    \textbf{Altura máxima} & \textbf{Criterio} & \textbf{Accuracy (training)} & \textbf{Accuracy (validación)} \\ \hline
    3 & Gini     & 0.813529 & 0.647059 \\ \hline
    5 & Gini     & 0.921176 & 0.670588 \\ \hline
    Infinito & Gini     & 1.000000 & 0.663529 \\ \hline
    3 & Entropía & 0.768235 & 0.665882 \\ \hline
    5 & Entropía & 0.892941 & 0.656471 \\ \hline
    Infinito & Entropía & 1.000000 & 0.647059 \\ \hline
    \end{tabular}
    \caption*{Resultados de accuracy para diferentes alturas y criterios.}
    \label{tab:accuracy_arboles}
\end{table}
    
Con ambos criterios se puede ver que al no limitar la altura máxima del árbol (o sea, altura máxima = Infinito),
el accuracy de entrenamiento es 1, lo cual indica que el modelo ajusta a perfectamente a los datos de entrenamiento y sabe clasificar instancias ya vistas. Sin embargo, en todos los casos el accuracy de validación es menor que lo que conseguiría un modelo que predice la clase más frecuente (siendo que tenemos 68\% de instancias de clase negativa), y en todos los casos se tiene una brecha amplia con el score de entrenamiento. Esto nos da a entender que los modelos planteados están sobreajustados y no pueden generalizar a partir de lo que aprendieron en entrenamiento. 

En el caso de usar Gini, al aumentar la altura máxima del árbol de 3 a 5, el accuracy de validación mejora. Sin embargo, también aumentan el accuracy de entrenamiento y la brecha entre los dos puntajes. En el caso de entropía, con el aumento de altura entre 3 y 5, se ve incluso una disminución en el accuracy de validación, sugiriendo una generalización incluso peor al separar los datos bajo este criterio con un nivel más.