\section*{Conclusiones}

Finalmente, con todos los analisis realizados, hicimos la validación final usando los datos de test 
que nos separamos al principio. Para esta evaluación probamos con los mejores modelos 
de \textbf{RandomForest}, \textbf{K-NN} y \textbf{SVM}, los resultados fueron los siguientes:
\begin{table}[H]
    \centering
    \begin{tabular}{l|cc}
        \hline
        \textbf{Modelo} & \textbf{AUC-ROC} & \textbf{Accuracy} \\ \hline
        RandomForest & 0.8039 & 0.7067 \\ \hline
        KNN & 0.8633 & 0.7867 \\ \hline
        SVM & 0.9204 & 0.88 \\ \hline
    \end{tabular}
    \caption*{Resultados finales con datos de test}
    \label{tab:resultados}
\end{table}

Los mejores resultados se obtienen usando \textbf{SVM}, así que el \textit{held\_out} final lo
realizamos con este modelo.

Notemos que el \textit{AUC-ROC} de test en SVM nos dio ligeramente más alto que en el Cross Validation.
Nuestra hipótesis de porqué está ocurriendo esto es que el modelo aprendió muy bien a separar las clases,
y dado que los datos de test son pocos, ya por cuestiones de azar, el modelo logró generalizar mejor en test.
La distribución de clases es igual en train y test, por lo que se comporta como esperamos.

El hecho de que los mejores modelos sean los que encuentran patrones no lineales
en los datos, como \textbf{SVM} y \textbf{KNN}, nos indica que el dataset tiene una separación no lineal entre las clases.
En \textbf{SVM} el kernel radial es el que mejor clasifica, esto nos indica que los datos pueden estar
distribuidos de una forma similar a una normal.
Esto tambien aplica en \textbf{LDA} y \textbf{Naive Bayes} que asumen esta condición sobre los datos.

\subsection*{Oversampling}
Nos encontramos frente a un dataset desbalanceado,
es muy posible que el modelo aprenda a predecir la clase mayoritaria y no generalice bien.
Para evitar esto, podemos aplicar oversampling a la clase minoritaria.
Inyectando datos sintéticos a la clase minoritaria para que haya una distribución equitativa de clases en el entrenamineto.

Por el momento reconocemos este problema y lo dejamos como tarea a futuro, en este trabajo nos centramos en entender como distintos modelos
y configuraciones especificas varian la performance y predicciones esperadas.